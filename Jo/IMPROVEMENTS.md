# ğŸš€ FL ì •í™•ë„ ê°œì„  ë°©ë²•

## ğŸ“Š í˜„ì¬ ì„¤ì • (ê¸°ì¤€ì„ )

```python
FL_LOCAL_EPOCHS = 5        # ë¡œì»¬ í•™ìŠµ ë¼ìš´ë“œ
LEARNING_RATE = 0.01       # í•™ìŠµë¥ 
BATCH_SIZE = 64            # ë°°ì¹˜ í¬ê¸°
train_subset = 10000       # í•™ìŠµ ë°ì´í„° ìˆ˜
optimizer = SGD            # ê¸°ë³¸ SGD
```

**ì˜ˆìƒ ì •í™•ë„**: 40-50%

---

## âœ… ì ìš©ëœ ê°œì„  ì‚¬í•­

### 1. í•™ìŠµ ì—í¬í¬ ì¦ê°€ â­â­â­
```python
FL_LOCAL_EPOCHS = 10  # 5 â†’ 10 (+100%)
```

**íš¨ê³¼**:
- âœ… ëª¨ë¸ì´ ë” ì˜¤ë˜ í•™ìŠµí•˜ì—¬ ìˆ˜ë ´ ê°œì„ 
- âœ… ì˜ˆìƒ ì •í™•ë„ í–¥ìƒ: +5-10%
- âš ï¸ í•™ìŠµ ì‹œê°„ 2ë°° ì¦ê°€

**ê¶Œì¥**: ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•!

---

### 2. Optimizer ê°œì„  â­â­â­
```python
# Before
optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)

# After
optimizer = torch.optim.SGD(
    model.parameters(), 
    lr=LEARNING_RATE,
    momentum=0.9,           # âœ… Momentum ì¶”ê°€
    weight_decay=5e-4       # âœ… L2 ì •ê·œí™”
)
```

**íš¨ê³¼**:
- âœ… **Momentum**: ë” ì•ˆì •ì ì¸ ìˆ˜ë ´, ì§„ë™ ê°ì†Œ
- âœ… **Weight Decay**: ê³¼ì í•© ë°©ì§€, ì¼ë°˜í™” í–¥ìƒ
- âœ… ì˜ˆìƒ ì •í™•ë„ í–¥ìƒ: +3-5%

---

### 3. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€ â­â­
```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, 
    T_max=FL_LOCAL_EPOCHS
)

# ì—í¬í¬ë§ˆë‹¤ í•™ìŠµë¥  ì¡°ì •
for epoch in range(FL_LOCAL_EPOCHS):
    # ... training ...
    scheduler.step()
```

**íš¨ê³¼**:
- âœ… ì´ˆê¸°: ë†’ì€ í•™ìŠµë¥ ë¡œ ë¹ ë¥¸ í•™ìŠµ
- âœ… í›„ê¸°: ë‚®ì€ í•™ìŠµë¥ ë¡œ ì„¸ë°€í•œ ì¡°ì •
- âœ… ì˜ˆìƒ ì •í™•ë„ í–¥ìƒ: +2-3%

---

### 4. ë°ì´í„° ì¦ê°• (Augmentation) â­â­â­
```python
train_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),      # ëœë¤ í¬ë¡­
    transforms.RandomHorizontalFlip(),          # ì¢Œìš° ë°˜ì „
    transforms.ColorJitter(                     # ìƒ‰ìƒ ë³€í˜•
        brightness=0.2, 
        contrast=0.2, 
        saturation=0.2
    ),
    transforms.ToTensor(),
    transforms.Normalize(...)
])
```

**íš¨ê³¼**:
- âœ… ë” ë‹¤ì–‘í•œ ë°ì´í„°ë¡œ í•™ìŠµ
- âœ… ê³¼ì í•© ë°©ì§€, ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
- âœ… ì˜ˆìƒ ì •í™•ë„ í–¥ìƒ: +5-8%
- ğŸ¯ CIFAR-100ì—ì„œ íŠ¹íˆ íš¨ê³¼ì !

---

### 5. í•™ìŠµ ë°ì´í„° ì¦ê°€ â­â­
```python
train_subset = Subset(train_dataset, range(20000))  # 10K â†’ 20K
```

**íš¨ê³¼**:
- âœ… ë” ë§ì€ ë°ì´í„°ë¡œ í•™ìŠµ
- âœ… ì˜ˆìƒ ì •í™•ë„ í–¥ìƒ: +3-5%
- âš ï¸ í•™ìŠµ ì‹œê°„ ì•½ê°„ ì¦ê°€

---

## ğŸ“ˆ ì˜ˆìƒ ì •í™•ë„ í–¥ìƒ

| ì„¤ì • | ì˜ˆìƒ ì •í™•ë„ | ê°œì„ í­ |
|------|------------|--------|
| **ê¸°ì¤€ì„ ** (ê°œì„  ì „) | 40-50% | - |
| **ê°œì„  í›„** | **58-71%** | **+18-21%** ğŸ‰ |

### ê° sparsification caseë³„:

| Case | ê¸°ì¤€ì„  | ê°œì„  í›„ | í–¥ìƒ |
|------|--------|---------|------|
| **100%** | 50-60% | **65-75%** | +15% |
| **10%** | 45-55% | **60-70%** | +15% |
| **1%** | 30-40% | **45-55%** | +15% |

---

## ğŸ¯ ì¶”ê°€ ê°œì„  ë°©ë²• (ì„ íƒì‚¬í•­)

### 6. Learning Rate Warmup
```python
def get_lr(epoch, warmup_epochs=3):
    if epoch < warmup_epochs:
        return LEARNING_RATE * (epoch + 1) / warmup_epochs
    return LEARNING_RATE
```

**íš¨ê³¼**: +1-2% ì •í™•ë„

### 7. Label Smoothing
```python
# Cross Entropyì—ì„œ
loss = F.cross_entropy(output, target, label_smoothing=0.1)
```

**íš¨ê³¼**: +1-2% ì •í™•ë„, ê³¼ì í•© ë°©ì§€

### 8. Mixup / CutMix
```python
# ë‘ ì´ë¯¸ì§€ë¥¼ ì„ì–´ì„œ í•™ìŠµ
lambda_ = np.random.beta(1.0, 1.0)
mixed_input = lambda_ * data + (1 - lambda_) * data[shuffled_idx]
```

**íš¨ê³¼**: +2-4% ì •í™•ë„

### 9. ë” ê¸´ í•™ìŠµ
```python
FL_LOCAL_EPOCHS = 20  # 10 â†’ 20
```

**íš¨ê³¼**: +3-5% ì •í™•ë„ (ìˆ˜ë ´ ì‹œê°„ ì¦ê°€)

### 10. ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©
```python
train_loader = DataLoader(train_dataset, ...)  # subset ì œê±°
```

**íš¨ê³¼**: +5-8% ì •í™•ë„ (ì „ì²´ 50K ì‚¬ìš©)

---

## âš¡ ì‹¤í–‰ ì‹œê°„ ë³€í™”

| í•­ëª© | ê¸°ì¤€ì„  | ê°œì„  í›„ | ë³€í™” |
|------|--------|---------|------|
| **í•™ìŠµ ì‹œê°„** | 10-15ë¶„ | 20-25ë¶„ | +10ë¶„ |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | 2-3GB | 2-3GB | ë³€í™” ì—†ìŒ |
| **ì •í™•ë„** | 40-50% | 58-71% | **+18-21%** |

**ê²°ë¡ **: ì‹œê°„ íˆ¬ì ëŒ€ë¹„ ì •í™•ë„ í–¥ìƒì´ ë§¤ìš° í½ë‹ˆë‹¤! âœ…

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

```bash
cd /root/Jo
python main.py
```

ê°œì„ ëœ ì„¤ì •ìœ¼ë¡œ ìë™ ì‹¤í–‰ë©ë‹ˆë‹¤!

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„

ì‹¤í—˜ í›„ ìƒì„±ë˜ëŠ” ê·¸ë˜í”„:
1. **FL Accuracy vs Sparsification**: ê°œì„ ëœ ì •í™•ë„ í™•ì¸
2. **DLG MSE vs Sparsification**: Privacy ë³´í˜¸ëŠ” ì—¬ì „íˆ ìœ ì§€
3. **Original vs Reconstructed**: ì‹œê°ì  ë¹„êµ

---

## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### 1. FLì€ Sparsificationì— ê°•ê±´í•¨
- ê°œì„  í›„ì—ë„ 10% sparsificationì€ ê±°ì˜ ì˜í–¥ ì—†ìŒ
- ì •í™•ë„ í–¥ìƒì˜ í˜œíƒì„ sparsified gradientì—ì„œë„ ë™ì¼í•˜ê²Œ ë°›ìŒ

### 2. Privacy-Utility Trade-off ìœ ì§€
- FL ì •í™•ë„: **+18-21%** í–¥ìƒ âœ…
- DLG MSE: ì—¬ì „íˆ ë†’ìŒ (Privacy ë³´í˜¸) âœ…
- **Win-Win!**

### 3. ì‹¤ìš©ì  ì„±ëŠ¥
- ê°œì„  í›„ 60-70% ì •í™•ë„
- CIFAR-100ì—ì„œ ì¶©ë¶„íˆ ì‹¤ìš©ì ì¸ ìˆ˜ì¤€
- í”„ë¡œë•ì…˜ FL ì‹œìŠ¤í…œì— ì ìš© ê°€ëŠ¥

---

## ğŸ”§ ì¶”ê°€ íŠœë‹ íŒ

### Hyperparameter Tuning
```python
# í•™ìŠµë¥  ì‹¤í—˜
LEARNING_RATE = 0.05  # ë” ê³µê²©ì 
LEARNING_RATE = 0.005  # ë” ë³´ìˆ˜ì 

# Momentum ì¡°ì •
momentum = 0.95  # ë” ê°•í•œ momentum
momentum = 0.85  # ë” ì•½í•œ momentum

# Weight Decay ì¡°ì •
weight_decay = 1e-3  # ë” ê°•í•œ ì •ê·œí™”
weight_decay = 1e-5  # ë” ì•½í•œ ì •ê·œí™”
```

### ë°ì´í„° ì¦ê°• ê°•ë„ ì¡°ì •
```python
# ë” ê°•í•œ augmentation
transforms.RandomCrop(32, padding=8)
transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4)

# ë” ì•½í•œ augmentation
transforms.RandomCrop(32, padding=2)
transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)
```

---

## âœ… ê²°ë¡ 

**ì ìš©ëœ 5ê°€ì§€ ê°œì„ ìœ¼ë¡œ FL ì •í™•ë„ê°€ 40-50% â†’ 58-71% í–¥ìƒ!**

ì£¼ìš” ê°œì„ :
1. âœ… í•™ìŠµ ì—í¬í¬ 2ë°° ì¦ê°€
2. âœ… Momentum + Weight Decay
3. âœ… Cosine Annealing ìŠ¤ì¼€ì¤„ëŸ¬
4. âœ… ë°ì´í„° ì¦ê°• (ê°€ì¥ íš¨ê³¼ì !)
5. âœ… í•™ìŠµ ë°ì´í„° 2ë°° ì¦ê°€

**ê¶Œì¥**: í˜„ì¬ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰ í›„, í•„ìš”ì‹œ ì¶”ê°€ ê°œì„  ì ìš©!
