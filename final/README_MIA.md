# FedAvg Model Inversion Attack with Gradient Sparsification

μ΄ ν”„λ΅μ νΈλ” **Federated Learning (FedAvg)** ν™κ²½μ—μ„ **Model Inversion Attack (MIA)**μ„ μν–‰ν•λ©΄μ„ **Gradient Sparsification**μ„ μ μ©ν•μ—¬ νλΌλ―Έν„° λΉ„μ¨μ„ μ΅°μ ν•  μ μλ” μ‹μ¤ν…μ…λ‹λ‹¤.

## π“‹ κ°μ”

### μ£Όμ” νΉμ§•

1. **FedAvg ν™κ²½**: ν΄λΌμ΄μ–ΈνΈκ°€ λ΅μ»¬ λ°μ΄ν„°λ΅ λ¨λΈμ„ ν•™μµν• ν›„ κ·Έλλ””μ–ΈνΈλ¥Ό μ„λ²„λ΅ μ „μ†΅
2. **Model Inversion Attack (MIA)**: μ¤νμ‹νμ΄λ κ·Έλλ””μ–ΈνΈλ΅λ¶€ν„° μ›λ³Έ λ°μ΄ν„° λ³µμ› μ‹λ„
3. **Gradient Sparsification**: μƒμ„ N% νλΌλ―Έν„°λ§ μ „μ†΅ν•μ—¬ ν†µμ‹  λΉ„μ© κ°μ† λ° ν”„λΌμ΄λ²„μ‹ ν–¥μƒ
4. **μ μ—°ν• λΉ„μ¨ μ μ–΄**: 0.01(1%)λ¶€ν„° 1.0(100%)κΉμ§€ μμ λ΅­κ² μ΅°μ  κ°€λ¥

### νλΌλ―Έν„° λΉ„μ¨ μ„¤μ • μμ‹

```
--sparsity 1.0   β†’ μƒμ„ 100% (μ „μ²΄ νλΌλ―Έν„° μ „μ†΅ - κΈ°μ¤€μ„ )
--sparsity 0.5   β†’ μƒμ„ 50% (μ λ°λ§ μ „μ†΅)
--sparsity 0.1   β†’ μƒμ„ 10% (10%λ§ μ „μ†΅)
--sparsity 0.05  β†’ μƒμ„ 5% (μƒμ„ 5%λ§ μ „μ†΅)
--sparsity 0.01  β†’ μƒμ„ 1% (1%λ§ μ „μ†΅ - κ°•λ ¥ν• μ••μ¶•)
```

## π€ λΉ λ¥Έ μ‹μ‘

### κΈ°λ³Έ μ‚¬μ©λ²•

```bash
# μƒμ„ 5% νλΌλ―Έν„°λ§ μ „μ†΅ν•λ” MIA κ³µκ²©
python fedavg_mia.py --sparsity 0.05 --index 25

# μƒμ„ 10% νλΌλ―Έν„° μ „μ†΅ (λ” μ μ€ μ••μ¶•)
python fedavg_mia.py --sparsity 0.1 --index 25

# μ „μ²΄ νλΌλ―Έν„° μ „μ†΅ (κΈ°μ¤€μ„ /baseline)
python fedavg_mia.py --sparsity 1.0 --index 25
```

### μ—¬λ¬ λΉ„μ¨ ν• λ²μ— μ‹¤ν–‰

```bash
# κΈ°λ³Έ μ„¤μ • (1.0, 0.5, 0.1, 0.05, 0.01)
python batch_mia_experiments.py --index 25

# μ»¤μ¤ν…€ λΉ„μ¨ μ„¤μ •
python batch_mia_experiments.py --index 25 --sparsities 1.0 0.2 0.05 0.01
```

### κ²°κ³Ό λΉ„κµ λ° λ¶„μ„

```bash
# λΉ„κµ λ¶„μ„ λ° μ‹κ°ν™” μƒμ„±
python compare_mia_results.py --sparsities 1.0 0.1 0.05 0.01

# μ»¤μ¤ν…€ μ¶λ ¥ μ΄λ¦„
python compare_mia_results.py --sparsities 1.0 0.5 0.1 0.05 0.01 --output results/mia_analysis
```

## π“ μƒμ„Έ λ…λ Ήμ–΄

### `fedavg_mia.py` - λ©”μΈ κ³µκ²© μ¤ν¬λ¦½νΈ

**κΈ°λ³Έ μΈμ:**

```bash
python fedavg_mia.py [options]
```

**μ£Όμ” μµμ…:**

| μµμ… | κΈ°λ³Έκ°’ | μ„¤λ… |
|------|--------|------|
| `--index` | 25 | CIFAR-10 λ°μ΄ν„°μ…‹μ λ€μƒ μ΄λ―Έμ§€ μΈλ±μ¤ |
| `--sparsity` | 1.0 | κ·Έλλ””μ–ΈνΈ μ μ§€ λΉ„μ¨ (0.01~1.0) |
| `--local_epochs` | 1 | λ΅μ»¬ ν•™μµ μ—ν¬ν¬ μ |
| `--local_lr` | 0.01 | λ΅μ»¬ ν•™μµλ¥  |
| `--mia_iters` | 300 | MIA μµμ ν™” λ°λ³µ νμ |
| `--seed` | 1234 | λ‚μ μ‹λ“ |

**μ‹¤ν–‰ μμ‹:**

```bash
# κΈ°λ³Έ μ„¤μ •
python fedavg_mia.py

# μ»¤μ¤ν…€ μ„¤μ •: μƒμ„ 5%, 500 λ°λ³µ
python fedavg_mia.py --sparsity 0.05 --mia_iters 500

# λ‹¤λ¥Έ μ΄λ―Έμ§€, λ” λ§μ€ λ΅μ»¬ μ—ν¬ν¬
python fedavg_mia.py --index 42 --sparsity 0.1 --local_epochs 5

# κ³ μ •λ λ‚μ μ‹λ“λ΅ μ¬ν„ κ°€λ¥ν• μ‹¤ν—
python fedavg_mia.py --sparsity 0.01 --seed 42
```

### `batch_mia_experiments.py` - λ°°μΉ μ‹¤ν—

**κΈ°λ³Έ μΈμ:**

```bash
python batch_mia_experiments.py [options]
```

**μ£Όμ” μµμ…:**

| μµμ… | κΈ°λ³Έκ°’ | μ„¤λ… |
|------|--------|------|
| `--index` | 25 | λ€μƒ μ΄λ―Έμ§€ μΈλ±μ¤ |
| `--sparsities` | [1.0, 0.5, 0.1, 0.05, 0.01] | ν…μ¤νΈν•  sparsity λΉ„μ¨ λ©λ΅ |
| `--mia_iters` | 300 | κ° μ‹¤ν—μ MIA λ°λ³µ νμ |
| `--local_epochs` | 1 | λ΅μ»¬ ν•™μµ μ—ν¬ν¬ |
| `--seed` | 1234 | λ‚μ μ‹λ“ |

**μ‹¤ν–‰ μμ‹:**

```bash
# κΈ°λ³Έ μ„¤μ • (5κ°€μ§€ sparsity λΉ„μ¨)
python batch_mia_experiments.py

# μ»¤μ¤ν…€ λΉ„μ¨: 100%, 10%, 1%λ§ ν…μ¤νΈ
python batch_mia_experiments.py --sparsities 1.0 0.1 0.01

# λ” μμ„Έν• λ¶„μ„: λ” λ§μ€ λ°λ³µ
python batch_mia_experiments.py --mia_iters 500 --local_epochs 2
```

### `compare_mia_results.py` - κ²°κ³Ό λΉ„κµ λ¶„μ„

**κΈ°λ³Έ μΈμ:**

```bash
python compare_mia_results.py [options]
```

**μ£Όμ” μµμ…:**

| μµμ… | κΈ°λ³Έκ°’ | μ„¤λ… |
|------|--------|------|
| `--sparsities` | [1.0, 0.5, 0.1, 0.05, 0.01] | λΉ„κµν•  sparsity λΉ„μ¨ λ©λ΅ |
| `--output` | mia_comparison | μ¶λ ¥ νμΌ ν”„λ¦¬ν”½μ¤ |

**μ‹¤ν–‰ μμ‹:**

```bash
# κΈ°λ³Έ μ„¤μ •μΌλ΅ λΉ„κµ λ¶„μ„
python compare_mia_results.py

# νΉμ • λΉ„μ¨λ§ λΉ„κµ
python compare_mia_results.py --sparsities 1.0 0.05 0.01

# μ»¤μ¤ν…€ μ¶λ ¥ νμΌλ…
python compare_mia_results.py --output results/privacy_analysis
```

## π“ μ¶λ ¥ νμΌ μ„¤λ…

### λ©”μΈ μ‹¤ν–‰ κ²°κ³Ό (`fedavg_mia.py`)

κ° μ‹¤ν—λ§λ‹¤ λ‹¤μ νμΌλ“¤μ΄ μƒμ„±λ©λ‹λ‹¤:

```
mia_ground_truth.png              # μ›λ³Έ μ΄λ―Έμ§€
mia_initial_dummy.png             # μ΄κΈ° μ„μ λ…Έμ΄μ¦
mia_progress_sparsity_5.png       # λ³µμ› κ³Όμ • (10λ‹¨κ³„)
mia_loss_sparsity_5.png           # μ†μ‹¤ ν•¨μ κ³΅μ„  (λ΅κ·Έ μ¤μΌ€μΌ)
mia_final_sparsity_5.png          # μµμΆ… λΉ„κµ (μ›λ³Έ vs λ³µμ›)
mia_gradient_dist_sparsity_5.png  # κ·Έλλ””μ–ΈνΈ λ¶„ν¬ νμ¤ν† κ·Έλ¨
```

### λΉ„κµ λ¶„μ„ κ²°κ³Ό (`compare_mia_results.py`)

```
mia_comparison_tradeoff.png       # ν”„λΌμ΄λ²„μ‹-μ ν‹Έλ¦¬ν‹° νΈλ μ΄λ“μ¤ν”„
mia_comparison_compression.png    # μ••μ¶• ν¨μ¨μ„±
mia_comparison_difficulty.png     # κ³µκ²© λ‚μ΄λ„ λ¶„μ„
mia_comparison_summary_table.png  # μ”μ•½ ν…μ΄λΈ”
mia_comparison_report.txt         # μƒμ„Έ ν…μ¤νΈ λ¦¬ν¬νΈ
```

## π” κΈ°μ  μƒμ„Έ

### Sparsification λ©”μ»¤λ‹μ¦

```python
def sparsify_gradients(gradients, ratio):
    """
    μƒμ„ k% κ·Έλλ””μ–ΈνΈλ§ μ μ§€, λ‚λ¨Έμ§€λ” 0μΌλ΅ μ„¤μ •
    
    1. λ¨λ“  κ·Έλλ””μ–ΈνΈλ¥Ό μ λ“κ°’ κΈ°μ¤€μΌλ΅ μ •λ ¬
    2. μƒμ„ kκ°λ§ μ„ νƒ (k = μ „μ²΄ * ratio)
    3. μ„κ³„κ°’ μ΄μƒμΈ κ°’λ§ μ μ§€, λ‚λ¨Έμ§€λ” 0
    """
```

**μμ‹:**
- `ratio=0.05`: 100,000 νλΌλ―Έν„° μ¤‘ 5,000κ°λ§ μ μ§€
- `ratio=0.1`: 100,000 νλΌλ―Έν„° μ¤‘ 10,000κ° μ μ§€
- `ratio=1.0`: μ „μ²΄ 100,000κ° λ¨λ‘ μ μ§€

### Model Inversion Attack λ‹¨κ³„

1. **FedAvg λ΅μ»¬ ν•™μµ**
   - ν΄λΌμ΄μ–ΈνΈκ°€ μ›λ³Έ μ΄λ―Έμ§€λ΅ λ¨λΈ ν•™μµ
   - μ†μ‹¤ ν•¨μ κΈ°μΈκΈ° κ³„μ‚°

2. **κ·Έλλ””μ–ΈνΈ μ¤νμ‹ν”ΌμΌ€μ΄μ…**
   - μƒμ„ N% νλΌλ―Έν„°λ§ μ μ§€
   - κ³µκ²©μκ°€ μ΄ μ¤νμ‹νμ΄λ κ·Έλλ””μ–ΈνΈ μμ‹ 

3. **Model Inversion (κ³µκ²©)**
   - μ„μ λ…Έμ΄μ¦λ΅ μ‹μ‘
   - μ¤νμ‹νμ΄λ κ·Έλλ””μ–ΈνΈμ™€ μΌμΉν•λ„λ΅ μµμ ν™”
   - LBFGS μµν‹°λ§μ΄μ € μ‚¬μ©

4. **λ©”νΈλ¦­ κ³„μ‚°**
   - MSE (Mean Squared Error)
   - PSNR (Peak Signal-to-Noise Ratio)
   - λ μ΄λΈ” λ³µμ› μ„±κ³µ μ—¬λ¶€

## π“ μ‹¤ν— μ‹λ‚λ¦¬μ¤

### μ‹λ‚λ¦¬μ¤ 1: ν”„λΌμ΄λ²„μ‹ vs μ ν‹Έλ¦¬ν‹° νΈλ μ΄λ“μ¤ν”„

```bash
# 5κ°€μ§€ sparsity μμ¤€ λΉ„κµ
python batch_mia_experiments.py --sparsities 1.0 0.5 0.1 0.05 0.01
python compare_mia_results.py --sparsities 1.0 0.5 0.1 0.05 0.01
```

**μμƒ κ²°κ³Ό:**
- Sparsity 1.0 (100%): κ³µκ²© μ„±κ³µλ¥  λ†’μ, ν”„λΌμ΄λ²„μ‹ λ‚®μ
- Sparsity 0.01 (1%): κ³µκ²© μ‹¤ν¨ κ°€λ¥μ„± λ†’μ, ν”„λΌμ΄λ²„μ‹ λ†’μ

### μ‹λ‚λ¦¬μ¤ 2: μµμ  μ••μ¶• λΉ„μ¨ μ°ΎκΈ°

```bash
# λ” μ΄μ΄ν• λ²”μ„ ν…μ¤νΈ
python batch_mia_experiments.py \
  --sparsities 1.0 0.5 0.2 0.1 0.05 0.02 0.01
```

### μ‹λ‚λ¦¬μ¤ 3: λ‹¤μ–‘ν• μ΄λ―Έμ§€λ΅ κ°•κ±΄μ„± ν…μ¤νΈ

```bash
for idx in 10 25 42 99 123; do
  python fedavg_mia.py --index $idx --sparsity 0.05
done
```

## π― μ„±λ¥ μ§€ν‘

### λ³µμ› ν’μ§ μ§€ν‘

- **MSE**: κ°’μ΄ μ‘μ„μλ΅ μΆ‹μ (λ²”μ„: 0~1)
- **PSNR**: κ°’μ΄ ν΄μλ΅ μΆ‹μ (μΌλ°μ μΌλ΅: 20~40 dB)
- **λ μ΄λΈ” μ •ν™•λ„**: μ›λ³Έ λ μ΄λΈ”κ³Όμ μΌμΉ μ—¬λ¶€

### ν”„λΌμ΄λ²„μ‹ μ§€ν‘

- **Sparsity Ratio**: λ‚®μ„μλ΅ λ” λ§μ€ ν”„λΌμ΄λ²„μ‹ λ³΄νΈ
- **μ••μ¶•λ¥ **: λ„¤νΈμ›ν¬ λ€μ—­ν­ μ μ•½ μ •λ„
- **κ³µκ²© λ‚μ΄λ„**: λ†’μ„μλ΅ λ°μ΄ν„° λ³µμ›μ΄ μ–΄λ ¤μ›€

## π”§ μ»¤μ¤ν„°λ§μ΄μ μ΄μ…

### λ¨λΈ λ³€κ²½

`models/vision.py`μ—μ„ λ‹¤λ¥Έ λ¨λΈ μ‚¬μ© κ°€λ¥:

```python
from models.vision import ResNet18, ResNet50, LeNet

# λ‹¤λ¥Έ λ¨λΈ ν…μ¤νΈ
net = ResNet50().to(device)  # ResNet18 λ€μ‹  ResNet50
```

### λ°μ΄ν„°μ…‹ λ³€κ²½

```python
# CIFAR-10 λ€μ‹  CIFAR-100
dst = datasets.CIFAR100("~/.torch", download=True)

# λλ” μ»¤μ¤ν…€ λ°μ΄ν„°μ…‹
from torchvision.datasets import ImageNet
dst = ImageNet("./data", split='train')
```

### μ •κ·ν™” νλΌλ―Έν„° μ΅°μ •

```python
# Total Variation κ°€μ¤‘μΉ (ν° κ°’ = λ” λ¶€λ“λ¬μ΄ μ΄λ―Έμ§€)
MIA_TV_WEIGHT = 0.01  # κΈ°λ³Έ: 0.001

# L2 μ •κ·ν™” (ν° κ°’ = λ” μ‘μ€ κ°’μΌλ΅ μ μ•½)
MIA_L2_WEIGHT = 0.001  # κΈ°λ³Έ: 0.0001
```

## π“ μ½”λ“ κµ¬μ΅°

```
DLG/sparsification/
β”β”€β”€ fedavg_mia.py                 # λ©”μΈ: FedAvg + MIA + Sparsification
β”β”€β”€ batch_mia_experiments.py      # λ°°μΉ μ‹¤ν— μλ™ν™”
β”β”€β”€ compare_mia_results.py        # κ²°κ³Ό λΉ„κµ λ° μ‹κ°ν™”
β”β”€β”€ utils.py                      # μ ν‹Έλ¦¬ν‹° ν•¨μ (λ μ΄λΈ” λ³€ν™ λ“±)
β”β”€β”€ models/
β”‚   β””β”€β”€ vision.py                 # ResNet18, LeNet λ“± λ¨λΈ μ •μ
β””β”€β”€ README.md                     # μ΄ νμΌ

```

## π› νΈλ¬λΈ”μν…

### λ©”λ¨λ¦¬ λ¶€μ΅± μ—λ¬

```bash
# MIA λ°λ³µ νμ κ°μ†
python fedavg_mia.py --sparsity 0.05 --mia_iters 100

# λλ” λ°°μΉ ν¬κΈ° κ°μ† (μ½”λ“ λ‚΄λ¶€ μμ •)
```

### λλ¦° μ‹¤ν–‰ μ†λ„

```bash
# GPU μ‚¬μ© ν™•μΈ
python -c "import torch; print(torch.cuda.is_available())"

# LBFGS λ°λ³µ νμ κ°μ†
python fedavg_mia.py --mia_iters 200
```

### λ³µμ› μ‹¤ν¨ (μ†μ‹¤ κ°’μ΄ κ°μ†ν•μ§€ μ•μ)

```bash
# μ •κ·ν™” νλΌλ―Έν„° μ΅°μ •
# fedavg_mia.py λ‚΄λ¶€μ—μ„:
MIA_TV_WEIGHT = 0.01  # λ” ν° κ°’
MIA_L2_WEIGHT = 0.001

# λλ” ν•™μµλ¥  μ΅°μ •
# optimizer = torch.optim.LBFGS([...], lr=0.5)  # κΈ°λ³Έ: 1.0
```

## π“ μ°Έκ³  μλ£

### κ΄€λ ¨ λ…Όλ¬Έ

- **Deep Leakage from Gradients** (DLG)
  - Zhu et al., NeurIPS 2019
  - κ·Έλλ””μ–ΈνΈλ΅λ¶€ν„° κ°μΈμ •λ³΄ μ¶”μ¶

- **Federated Learning**
  - McMahan et al., AISTATS 2017
  - λ¶„μ‚° ν•™μµ λ° ν”„λΌμ΄λ²„μ‹

- **Gradient Sparsification**
  - ν†µμ‹  ν¨μ¨μ„± λ° ν”„λΌμ΄λ²„μ‹ ν–¥μƒ
  - μƒμ„-k μ„ νƒ λ©”μ»¤λ‹μ¦

## π’΅ μ£Όμ” μΈμ‚¬μ΄νΈ

1. **Sparsityκ°€ λ‚®μ„μλ΅ ν”„λΌμ΄λ²„μ‹ λ³΄νΈ μ¦κ°€**
   - 1%λ§ μ „μ†΅ β†’ κ³µκ²© λ‚μ΄λ„ λ§¤μ° λ†’μ

2. **ν”„λΌμ΄λ²„μ‹-μ ν‹Έλ¦¬ν‹° νΈλ μ΄λ“μ¤ν”„**
   - κ³Όλ„ν• μ••μ¶• β†’ λ¨λΈ μ„±λ¥ μ €ν•
   - μµμ μ  μ°ΎκΈ° ν•„μ”

3. **μƒμ„-k μ„ νƒ λ©”μ»¤λ‹μ¦μ ν¨κ³Ό**
   - μ λ“κ°’μ΄ ν° κ·Έλλ””μ–ΈνΈλ§ μ μ§€
   - μ‘μ€ κ°’(λ…Έμ΄μ¦) μ κ±°

## π“ λΌμ΄μ„ μ¤

μ΄ ν”„λ΅μ νΈλ” κµμ΅ λ° μ—°κµ¬ λ©μ μΌλ΅ μ κ³µλ©λ‹λ‹¤.

## π‘¤ μ‘μ„±μ

Hansung University - AI & ML Research Lab

---

**λ§μ§€λ§‰ μ—…λ°μ΄νΈ**: 2026λ…„ 2μ›”

**λ²„μ „**: 1.0
