Model Inversion Attacks (MIA): Research & Implementation
본 저장소는 모델 역공격(Model Inversion Attacks, MIA)에 대한 이론적 학습 내용과 관련 논문들을 정리하고, 이를 방어하기 위한 기법들을 구현하는 공간입니다. 특히 연합학습(Federated Learning) 환경에서의 Deep Leakage from Gradients (DLG) 공격과 이를 완화하기 위한 Gradient Sparsification 기법을 중점적으로 다룹니다.
